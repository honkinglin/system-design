# 设计 Twitter

我们来设计一个类似 Twitter 的社交网络服务。用户可以发布推文、关注他人并收藏推文。

难度级别：中等

## 1. 什么是 Twitter？
Twitter 是一个在线社交网络服务，用户可以发布和阅读长度为 140 字符的短消息，称为“推文”。注册用户可以发布和阅读推文，未注册用户只能阅读推文。用户可以通过网站、短信或移动应用访问 Twitter。

## 2. 系统的需求与目标
我们将设计一个简化版的 Twitter，包含以下需求：

### 功能性需求
1. 用户可以发布新推文。
2. 用户可以关注其他用户。
3. 用户可以将推文标记为收藏。
4. 系统应能创建并显示用户时间线，包含用户关注的所有人的热门推文。
5. 推文可以包含照片和视频。

### 非功能性需求
1. 系统需要高度可用。
2. 时间线生成的可接受延迟为 200 毫秒。
3. 在可用性优先的情况下，系统的强一致性要求可以适当降低；如果用户暂时看不到某条推文，可以接受。

### 拓展需求
1. 推文搜索。
2. 回复推文。
3. 热门话题——当前热点话题/搜索。
4. @用户标签。
5. 推文通知。
6. 关注建议。
7. “时刻”功能。

## 3. 容量估算与限制

假设我们有10亿用户，总共有2亿日活跃用户（DAU）。另外假设每天有1亿条新推文，每个用户平均关注200人。

**每天的点赞次数？**

如果每个用户平均每天点赞五次，那么我们将有：

`2亿用户 * 5 次点赞 => 10亿次点赞`

**系统每天会产生多少次推文查看？**

假设用户平均每天访问时间线两次，并访问其他五个人的页面。在每个页面上，用户看到20条推文，那么系统每天将产生280亿次推文查看：

`2亿日活跃用户 * ((2 + 5) * 20条推文) => 280亿次/天`

**存储估算**

假设每条推文有140个字符，存储字符时不进行压缩，每个字符需要2字节。假设每条推文需要30字节存储元数据（如ID、时间戳、用户ID等）。我们需要的总存储量为：

`1亿条 * (280 + 30)字节 => 30GB/天`

**我们五年的存储需求会是多少？**

用户数据、关注、点赞的数据需求是多少？我们将留给练习解答。

并非所有推文都包含媒体内容，假设平均每五条推文包含一张照片，每十条推文包含一个视频。再假设每张照片平均大小为200KB，每个视频大小为2MB。那么每天新增的媒体内容为24TB：

`(1亿/5 张照片 * 200KB) + (1亿/10 个视频 * 2MB) ≈ 24TB/天`

**带宽估算**

由于每天的总输入量为24TB，这相当于290MB/秒。

记住我们每天有280亿次推文查看。我们必须显示每条推文的照片（如果有照片），假设用户在时间线上每三次观看中有一次观看视频。因此，总输出量将是：
```
(280亿 * 280字节) / 86400秒 文本 => 93MB/秒 
+ (280亿/5 * 200KB ) / 86400秒 照片 => 13GB/秒 
+ (280亿/10/3 * 2MB ) / 86400秒 视频 => 22GB/秒  

总计约为 35GB/秒
```

## 4. 系统API

> 💡 一旦我们确定了需求，定义系统API是个好主意。这将明确系统的预期功能。

我们可以使用SOAP或REST API来提供服务的功能。以下是发布新推文的API定义示例：

`tweet(api_dev_key, tweet_data, tweet_location, user_location, media_ids, maximum_results_to_return)`

**参数：**

- `api_dev_key` (string): 已注册账号的API开发者密钥。此密钥用于根据分配的配额限制用户。
- `tweet_data` (string): 推文文本，通常最多为140个字符。
- `tweet_location` (string): 此推文涉及的可选位置（经度、纬度）。
- `user_location` (string): 添加推文的用户的可选位置（经度、纬度）。
- `media_ids` (number[]): 可选的媒体ID列表，用于关联到推文。（所有媒体如照片、视频等需单独上传）。

**返回值：** (string)

成功发布将返回访问该推文的URL，否则会返回相应的HTTP错误。

## 5. 高层系统设计

我们需要一个系统，能够高效地存储所有新增的推文（`100M/86400秒 ≈ 每秒1150条推文`）并读取已有推文（`28B/86400秒 ≈ 每秒325K条推文`）。从需求可以看出，这是一个以读操作为主的系统。

在高层架构上，我们需要多个应用服务器来处理所有请求，并在其前端设置负载均衡器以分配流量。在后端，我们需要一个高效的数据库，既能存储所有新增的推文，又能支持大量的读取。同时，我们需要一些文件存储空间来存储照片和视频。

![图7-1](/grokking/f7-1.png)

虽然预期的日写入负载为1亿条推文、读取负载为280亿条推文，但平均来看，系统每秒大约会接收1160条新推文和325K次读取请求。然而，这些流量在一天内的分布并不均匀，在高峰期，我们至少需要应对数千次写入请求和约100万次读取请求。设计系统架构时需考虑到这一点。

## 6. 数据库架构

我们需要存储关于用户、他们的推文、他们收藏的推文以及他们关注的人等数据。

![图7-2](/grokking/f7-2.png)

关于选择SQL还是NoSQL数据库来存储上述架构，请参见《设计Instagram》中的“数据库架构”部分。

## 7. 数据分片

由于我们每天有大量的新推文，而且读负载也非常高，我们需要将数据分布到多个机器上，以便高效地进行读写。我们有很多选项可以对数据进行分片，下面逐一介绍：

**基于UserID的分片**：我们可以尝试将一个用户的所有数据存储在一台服务器上。在存储时，我们可以将UserID传递给哈希函数，该函数将用户映射到一个数据库服务器，所有该用户的推文、收藏、关注等数据都存储在该服务器上。在查询用户的推文、关注或收藏时，我们可以通过哈希函数查询到该用户的数据存储位置，然后从中读取数据。该方法有几个问题：
1. 如果一个用户变得非常热门怎么办？可能会有大量的查询请求集中在该用户所在的服务器上。这种高负载将影响服务的性能。
2. 随着时间推移，一些用户的推文或关注人数可能远多于其他用户，导致数据分布不均。保持用户数据的均匀分布是非常困难的。

为了解决这些问题，我们可以重新分区/重新分配数据，或使用一致性哈希。

**基于TweetID的分片**：我们的哈希函数将每个TweetID映射到一个随机服务器，在该服务器上存储推文。要搜索推文，我们需要查询所有服务器，每个服务器会返回一组推文。一个集中式服务器将聚合这些结果并返回给用户。让我们来看一个生成时间线的例子，以下是系统生成用户时间线时需要执行的步骤：
1. 应用服务器找到用户关注的所有人。
2. 应用服务器将查询发送到所有数据库服务器，查找这些人的推文。
3. 每个数据库服务器会找到该用户的推文，按时间顺序排序并返回最新的推文。
4. 应用服务器将合并所有结果并再次排序，最终返回给用户最顶部的推文。

这种方法解决了“热门用户”问题，但与基于UserID的分片不同，我们需要查询所有数据库分区来查找某个用户的推文，这可能导致更高的延迟。

我们还可以通过在数据库服务器前引入缓存来进一步提升性能，将热门推文缓存起来。

**基于推文创建时间的分片**：根据推文创建时间进行分片，可以使我们快速获取所有的最新推文，而且只需要查询一小部分服务器。然而，这种方法的问题在于流量负载的不均衡。例如，在写入时，所有新的推文都会写入到同一台服务器，其他服务器则处于空闲状态。同样，在读取时，持有最新数据的服务器会承受比存储旧数据的服务器更高的负载。

**结合基于TweetID和推文创建时间的分片**：如果我们不单独存储推文创建时间，而是通过TweetID来反映创建时间，我们可以同时享有两者的优点。这样，我们可以快速找到最新的推文。为了实现这一点，我们必须使每个TweetID在系统中都是唯一的，并且每个TweetID都应该包含时间戳。

我们可以使用纪元时间（epoch time）来实现这一点。假设我们的TweetID将由两部分组成：第一部分表示纪元秒数，第二部分是一个自动递增的序列号。因此，生成一个新的TweetID时，我们可以取当前的纪元时间，并在其后附加一个递增的数字。然后我们可以根据这个TweetID来确定数据所在的分片，并将其存储在那里。

**TweetID的大小**：假设我们的纪元时间从今天开始，我们需要多少位来存储未来50年的秒数？

